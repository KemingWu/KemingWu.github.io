<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="shortcut icon" href="myIcon.ico">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

<!--<meta name="keywords" content="Keming Wu, Tsinghua University">-->
    <meta name="keywords" content="Keming Wu">
<meta name="description" content="Keming Wu's home page">
<meta name="google-site-verification" content="yy_3iiS_X6pJdegdwitJMrH0LRLHXwpjrV9RKLXxKjg" />
<meta name="google-site-verification" content="yy_3iiS_X6pJdegdwitJMrH0LRLHXwpjrV9RKLXxKjg" />
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Keming Wu's homepage</title>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-87320911-1', 'auto');
  ga('send', 'pageview');
<style>
  .left-align {
    text-align: left;
  }
</style>
	
</script>
</head>
<body>
<div id="layout-content" style="margin-top:25px">
<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">					
					<h1><font face="Arial"> Keming Wu </font></h1>  <h1><font face="Ê•∑‰Ωì"> Âê¥ÁßëÊòé </font></h1>
				</div>

				<h3><font face="Arial"> Ph.D. Student, Tsinghua University </font></h3>
				<p><font face="Arial">
					Beijing, China <br>
					Email: <a href="wukeming0608@gmail.com">wukeming0608@gmail.com</a> <a href="wukm25@mails.tsinghua.edu.cn">wukm25@mails.tsinghua.edu.cn</a><br>
                    <a href="https://github.com/KemingWu">[Github]</a>
                    <a href="https://scholar.google.com/citations?user=cOzwZBMAAAAJ&hl=en">[Google Scholar]</a>
                    <a href="https://www.semanticscholar.org/author/Keming-Wu/2383071656">[Semantic Scholar]</a>
                    <a href="https://x.com/Keming_Charles">[Twitter]</a>
				</font></p>
			</td>
			<td>
				<img src="person1.png" border="0" width="250"><br>
			</td>
		</tr><tr>
	</tr></tbody>
</table>

<h2><font face="Arial"> About </font></h2>
<!--    , advised by Assoc. Prof. <a href="https://binwangthss.github.io/">Bin Wang</a>-->
<!--    <p>-->
  I am a Ph.D. student at the <a href="https://www.thss.tsinghua.edu.cn/">School of Software</a> at <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a>. 
  Previously, I was a research intern at <a href="https://www.microsoft.com/en-us/research/group/visual-computing/">Visual Computing Group</a>, <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsoft Research Asia</a> from September, 2024 to April, 2025. Currently, my research interest include topics on <strong>deep generative models</strong> and their applications in <strong>Computer Vision</strong> and <strong>Language Models</strong>.
  <!-- I received my B.E. degree from the <a href="http://www.cse.cqu.edu.cn/">School of Big Data & Software Engineering</a>, Chongqing University in July, 2025 (<b>Ranking: 1/251</b>). -->

        <!-- I am a Ph.D. student at the <a href="https://www.thss.tsinghua.edu.cn/">School of Software</a> at <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a> and in close collaboration with Prof. <a href="https://wenhuchen.github.io/">Wenhu Chen</a> at the University of Waterloo. I received my B.E. degree from the <a href="http://www.cse.cqu.edu.cn/">School of Big Data & Software Engineering</a>, Chongqing University in July, 2025 (<b>Ranking: 1/251</b>). -->
<!--    </p>-->
<!--    <p>-->
<!--       I received my B.E. degree from <a href="http://www.cse.cqu.edu.cn/">School of Big Data & Software Engineering</a>, Chongqing University in July, 2025 (<b>Ranking: 1/251</b>).-->
<!--    </p>-->
<p class="left-align";>
    
    <p style="color: red;"> <strong>I‚Äôm currently actively seeking for Research Assistant, or internship positions related to any of the above topics. I‚Äôm also open to any possible discussions or collaborate opportunities. please feel free to contact me for further discussion and potential collaboration! </strong> </p>
      
    <!-- <p style="color: red;"> <strong> I‚Äôm also open to any possible discussions or collaborate opportunities. please feel free to contact me for further discussion and potential collaboration!
        </strong></p> -->

</font></p>

	
<!--<font color="red">Currently, I am working on domain generalization for semantic segmentation and object detection. I am looking for undergraduate or master students to engage in ongoing research papers. Don't hesitate to email me if you are interested.</font> <br><br>-->
	
<h2><font face="Arial"> News </h2>
<font size = "3"></font>
    <div style="height: 200px; overflow: auto;">
    <ul>
      <li> 2025.11 Two papers are released: <b>OpenMMReasoner</b>, and the other <b>LongVT</b>.</li>
      <li> 2025.10 One papers are released: Focusing on <b>generative image evaluation</b>.</li>
      <li> 2025.09 Two papers are released: one focusing on <b>image editing reward model</b>, and the other on <b>generative video evaluation</b>.</li>
      <li> 2025.08 One paper got accepted by <b>ACM MM 2025 Brave New Ideas Track (Oral)</b>.</li>
      <li> 2025.06 One paper about layout to image generation got accepted by <b>ICCV 2025</b> (<b>First Author</b>).</li>
      <li> 2025.05 One paper about multi-layer image generation is released.</li>
      <!-- <li> 2025.05 One paper got accepted by <b>ICML 2025</b>.</li> -->
<!--&lt;!&ndash;      <li> <p style="color: red;">2025.02  <strong> I have joined Shandong University as an Associate Researcher in C-FAIR.  </strong></p>&ndash;&gt; </li>-->
      <li> 2025.02 One paper got accepted by <b>CVPR 2025</b>.</li>
      <li> 2024.10 One paper about information fusion got accepted by IEEE Transactions on Systems, Man and Cybernetics: Systems (<b>CCF-B journal, First Author</b>).</li>
      <li> 2024.07 One paper got accepted by ACM MM 2024 (My first <b>CCF-A</b> conference paper, First Author. Congratulations!).</li>
      <li> 2024.01 One paper got accepted by Information Sciences (<b>CCF-B journal, First Author</b>).</li>
      <!-- <li> 2023.01 One paper got accepted by <strong>SDM 2023 </strong> -->
<!--         <li> 2022.12 One paper got accepted by <strong>AAAI 2023</strong> -->
<!--         <li> 2022.06 One paper got accepted by <strong>ECML-PKDD 2022</strong>
        <li> 2021.12 One paper got accepted by <strong>AAAI 2022</strong> -->
        <!-- <li> 2021.09 I have been awarded a student travel grant of CIKM 2021</li> -->
        <!-- <li> 2021.09 I have been selected as a PC member of AAAI 2022</li> -->
        <!-- <li> 2021.08 Two papers got accepted by <strong> CIKM 2021 </strong></li>
        <li> 2021.05 One paper got accepted by <strong> ICMR 2021 </strong></li>
        <li> 2021.03 One paper got accepted by <strong> IDA journal 2021 </strong></li>
        <li> 2021.01 Four papers got accepted by <strong> DASFAA 2021 </strong></li> -->
      <!--   <li> 2020.06 One paper got accepted by <strong> ECML-PKDD 2020 </strong></li> -->
      </ul>
</div>
<!--<p><font face="Arial"><a href="https://scholar.google.com/citations?user=1ogwTXkAAAAJ&hl=zh-CN">[Google Scholar]</a></font></p>-->
<h2><font face="Arial"> Selected Publications </font></h2>
<font size = "2"></font>
(* equal contribution)


<!--    <div style="height: 200px; overflow: auto;">-->
<ul>

  <li>
    <p>
      <strong>	OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe  </strong>
     <a href="https://arxiv.org/abs/2511.16334" target="_blank" style= "color:blue; text-decoration:none;">[PDF]</a>
        [<a href="https://evolvinglmms-lab.github.io/OpenMMReasoner/" target="_blank" style= "color:blue; text-decoration:none;">WebPage</a>]
        [<a href="https://github.com/EvolvingLMMs-Lab/OpenMMReasoner" target="_blank" style= "color:blue; text-decoration:none;">Code</a>]
        [<a href="https://huggingface.co/papers/2511.16334" target="_blank" style= "color:blue; text-decoration:none;">Daily Paper</a>]
    <p>
      Kaichen Zhang*, <b>Keming Wu*</b>, Zuhao Yang, Bo Li, Kairui Hu, Bin Wang, Ziwei Liu, Xingxuan Li, Lidong Bing
    </p>
    <p>
     <strong>  <p style="color: black;">Technical Report </p>  <p style="color: red;">üèÜ Top #1 Paper of the day at HuggingFace Daily Papers </p>  </strong>
    </p>
</li>

    <li>
    <p>
      <strong>	EditReward: A Human-Aligned Reward Model for Instruction-Guided Image Editing </strong>
<!--      <a href="https://arxiv.org/abs/2503.20672" target="_blank" style= "color:blue; text-decoration:none;">[PDF]</a>-->
        [<a href="https://tiger-ai-lab.github.io/EditReward" target="_blank" style= "color:blue; text-decoration:none;">WebPage</a>]
        [<a href="https://github.com/TIGER-AI-Lab/EditReward" target="_blank" style= "color:blue; text-decoration:none;">Code</a>]
    <p>
        <b>Keming Wu</b>, Sicong Jiang, Max Ku, Ping Nie, Minghao Liu, Wenhu Chen
    </p>
    <p>
     <strong>  <p style="color: black;">Technical Report </p>  </strong>
    </p>
</li>

    <li>
    <p>
      <strong>	Hybrid Layout Control for Diffusion Transformer: Fewer Annotations, Superior Aesthetics </strong>
<!--      <a href="https://arxiv.org/abs/2503.20672" target="_blank" style= "color:blue; text-decoration:none;">[PDF]</a>-->
        [<a href="https://hybrid-layout-msra.github.io/" target="_blank" style= "color:blue; text-decoration:none;">WebPage</a>]
        [<a href="https://github.com/KemingWu/HybridLayout" target="_blank" style= "color:blue; text-decoration:none;">Code</a>]
    <p>
        <b>Keming Wu</b>, Junwen Chen, Zhanhao Liang, Yinuo Wang, Ji Li, Chao Zhang, Bin Wang, Yuhui Yuan
    </p>
    <p>
     <strong>  <p style="color: red;">ICCV 2025</p>  </strong>
    </p>
</li>

  <li>
    <p>
      <strong>	BizGen: Advancing Article-level Visual Text Rendering for Infographics Generation </strong>
      <a href="https://arxiv.org/abs/2503.20672" target="_blank" style= "color:blue; text-decoration:none;">[PDF]</a>
        [<a href="https://bizgen-msra.github.io/" target="_blank" style= "color:blue; text-decoration:none;">WebPage</a>]
        [<a href="https://github.com/1230young/bizgen" target="_blank" style= "color:blue; text-decoration:none;">Code</a>]
    <p>
      Yuyang Peng, Shishi Xiao, <b>Keming Wu</b>, Qisheng Liao, Bohan Chen, Kevin Lin, Danqing Huang, Ji Li, Yuhui Yuan
    </p>
    <p>
     <strong>  <p style="color: red;">CVPR 2025</p>  </strong>
    </p>
</li>


  <li>
    <p>
      <strong>	RSC-SNN: Exploring the Trade-off Between Adversarial Robustness and Accuracy in Spiking Neural Networks via Randomized Smoothing Coding </strong>
      <a href="https://dl.acm.org/doi/abs/10.1145/3664647.3680639" target="_blank" style= "color:blue; text-decoration:none;">[PDF]</a>
        [<a href="https://github.com/KemingWu/RSC-SNN" target="_blank" style= "color:blue; text-decoration:none;">Code</a>]
    <p>
      <b>Keming Wu*</b>, Man Yao*, Yuhong Chou, Xuerui Qiu, Rui Yang, Bo Xu, Guoqi Li
    </p>
    <p>
     <strong>  <p style="color: red;">ACM MM 2024</p>  </strong>
    </p>
</li>

<li>
  <p>
    <strong>	PrismLayers: Open Data for High-Quality Multi-Layer Transparent Image Generative Models </strong>
    <a href="https://arxiv.org/abs/2505.22523" target="_blank" style= "color:blue; text-decoration:none;">[PDF]</a>
      [<a href="https://prism-layers.github.io/" target="_blank" style= "color:blue; text-decoration:none;">WebPage</a>]
<!--        [<a href="https://huggingface.co/datasets/artplus/PrismLayersPro" target="_blank" style= "color:blue; text-decoration:none;">Dataset</a>]-->
  <p>
      Junwen Chen*, Heyang Jiang*, <b>Keming Wu*</b>, Yanbin Wang*, Ji Li, Chao Zhang, Keiji Yanai, Dong Chen, Yuhui Yuan
  </p>
  <p>
    <strong>  <p style="color: black;">Technical Report </p>  </strong>
   </p>
</li>
</ul>
	
<!--</div>-->

<h2><font face="Arial"> Other Publications </font></h2>
<font size = "3">
    (* equal contribution)
<div style="height: 200px; overflow: auto;">
<ul>
  <!-- <li>
    <p>
      <strong>	A Rigorous Benchmark with Multidimensional Evaluation for Deep Research Agents: From Answers to Reports </strong>
      <a href="https://arxiv.org/abs/2510.02190" target="_blank" style= "color:blue; text-decoration:none;">[PDF]</a>
        [<a href="https://github.com/EVIGBYEN/RigorousBench" target="_blank" style= "color:blue; text-decoration:none;">WebPage</a>]
    <p>
      Yang Yao, Yixu Wang, Yuxuan Zhang, Yi Lu, Tianle Gu, Lingyu Li, Dingyi Zhao, <b>Keming Wu</b>, Haozhe Wang, Ping Nie, Yan Teng, Yingchun Wang
    </p>
    <p>
      <strong>  <p style="color: black;">Technical Report </p>  </strong>
     </p>
</li> -->

    <li>
    <p>
      <strong>	VideoScore2: Think before You Score in Generative Video Evaluation </strong>
      <a href="https://arxiv.org/abs/2509.22799" target="_blank" style= "color:blue; text-decoration:none;">[PDF]</a>
        [<a href="https://tiger-ai-lab.github.io/VideoScore2/" target="_blank" style= "color:blue; text-decoration:none;">WebPage</a>]
        [<a href="https://github.com/TIGER-AI-Lab/VideoScore2" target="_blank" style= "color:blue; text-decoration:none;">Code</a>]
    <p>
        Xuan He, Dongfu Jiang, Ping Nie, Minghao Liu, Zhengxuan Jiang, Mingyi Su, Wentao Ma, Junru Lin, Chun Ye, Yi Lu, <b>Keming Wu</b>, Benjamin Schneider, Quy Duc Do, Zhuofeng Li, Yiming Jia + 9 more authors
    </p>
    <p>
      <strong>  <p style="color: black;">Technical Report </p>  </strong>
     </p>
</li>

<li>
  <p>
    <strong>	Physics-Informed Representation Alignment for Sparse Radio-Map Reconstruction </strong>
    <a href="https://arxiv.org/abs/2501.19160" target="_blank" style= "color:blue; text-decoration:none;">[PDF]</a>
  <p>
      Haozhe Jia, Wenshuo Chen, Zhihui Huang, Lei Wang, Hongru Xiao, Nanqian Jia, <b>Keming Wu</b>, Songning Lai, Bowen Tian, Yutao Yue
  </p>
   <p style="color: red;">
    <strong>ACM MM 2025 Brave New Ideas TrackÔºàOralÔºâ</strong>
  </p>
</li>

<!-- <li>
  <p>
    <strong>	Modeling All-Atom Glycan Structures via Hierarchical Message Passing and Multi-Scale Pre-training </strong>
    <a href="https://arxiv.org/abs/2506.01376" target="_blank" style= "color:blue; text-decoration:none;">[PDF]</a>
      [<a href="https://github.com/kasawa1234/GlycanAA" target="_blank" style= "color:blue; text-decoration:none;">Code</a>]
  <p>
   Minghao Xu, Jiaze Song, <b>Keming Wu</b>, Xiangxin Zhou, Bin CUI, Wentao Zhang
  </p>
  <p>
   <strong>  <p style="color: red;">ICML 2025</p>  </strong>
  </p>
</li> -->

    <li>
    <p>
      <strong>	Free-T2M: Frequency Enhanced Text-to-Motion Diffusion Model With Consistency Loss </strong>
      <a href="https://arxiv.org/abs/2501.18232" target="_blank" style= "color:blue; text-decoration:none;">[PDF]</a>
        [<a href="https://github.com/Hxxxz0/Free-T2m" target="_blank" style= "color:blue; text-decoration:none;">Code</a>]
    <p>
        Wenshuo Chen, Haozhe Jia, Songning Lai, <b>Keming Wu</b>, Hongru Xiao, Lijie Hu, Yutao Yue
    </p>
    <p>
      <strong>  <p style="color: black;">Technical Report </p>  </strong>
     </p>
</li>

<li>
  <p>
    <strong>A Fractal-based Complex Belief Entropy for Uncertainty Measure in Complex Evidence Theory</strong>
    [<a href="https://ieeexplore.ieee.org/abstract/document/10770823/" target="_blank" style= "color:blue; text-decoration:none;">PDF</a>]
  </p>
  <p>
    <b>Keming Wu</b>, Fuyuan Xiao, Yi Zhang
  </p>
  <p style="color: red;">
    <strong>IEEE Transactions on Systems, Man and Cybernetics: Systems 2024</strong>
  </p>
</li>

<li>
  <p> 
    <strong>A Novel Quantum Belief Entropy for Uncertainty Measure in Complex Evidence Theory</strong>
    [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0020025523013294" target="_blank" style= "color:blue; text-decoration:none;">PDF</a>]
  </p>
  <p>
    <b>Keming Wu</b>, Fuyuan Xiao
  </p>
  <p style="color: red;">
    <strong>Information Sciences 2024</strong>
  </p>
</li>

</ul>
</div>

<h2><font face="Arial"> Honors &amp; Awards </h2>
	
<ul style="list-style-type:none">
<p style="line-height: 120%; margin-left: 0px; margin-top: 8pt; margin-bottom: 8pt;"> <font face="Arial" size="3">

    <!-- <li>Candidate for Person of the Year (<b>20/26,000+</b>,Highest honor for undergraduate students in CQU), China, Oct, 2024</li> -->
    <li>National Scholarship (<b>Three times</b>)</li>
    <!-- <li>National Scholarship (<b>Award Rate: 0.2% national-wide</b>) Ministry of Education, China, Oct, 2023</li> -->
    <!-- <li>National Scholarship (<b>Award Rate: 0.2% national-wide</b>) Ministry of Education, China, Oct, 2022</li> -->
    <!-- <li>National Scholarship for Encouragement, from NEU, October 2015,2016,2017</li> -->

</p>
</ul>

<h2><font face="Arial"> Work Experience </h2>
	
<ul style="list-style-type:none">
<p style="line-height: 120%; margin-left: 0px; margin-top: 8pt; margin-bottom: 8pt;"> <font face="Arial" size="3">
<li>[09/2024 - 4/2025]  Research Intern, Microsoft Research Asia supervised by Senior Researcher <a href="https://scholar.google.com/citations?user=PzyvzksAAAAJ&hl=en">Yuhui Yuan</a> and Principal Research Manager <a href="https://scholar.google.com/citations?hl=en&user=_fKSYOwAAAAJ">Dong Chen</a>.</li>
<li>[01/2024 - 9/2024]  Research Intern, Peking University supervised by Prof. <a href="https://zwt233.github.io/">Wentao Zhang</a>.</li>
<li>[04/2023 - 1/2024]  Research Intern, Institute of Automation, Chinese Academy of Sciences supervised by Prof. <a href="https://scholar.google.com/citations?user=qCfE--MAAAAJ&hl=en">Guoqi Li</a>.</li>
</p>
</ul>

<h2><font face="Arial"> Professional Activities </font></h2>
<ul style="list-style:none; padding-left:0; font-family:Arial;">
  <li><strong>Journal Reviewer</strong></li>
  <li><strong>Conference Reviewer</strong>
    <ul style="list-style:none; padding-left:18px; margin-top:4px;">
      <li style="position:relative; padding-left:14px; font-weight:normal; font-size:16px;">
        <span style="position:absolute; left:0; top:7px; width:5px; height:5px; background:#007acc; border-radius:50%;"></span>
        CVPR 2026, ICLR 2026
      </li>
      <!-- ÂêéÁª≠Áõ¥Êé•Â§çÂà∂ <li> Âç≥ÂèØ -->
      <!--
      <li style="position:relative; padding-left:14px; font-weight:normal; font-size:14px;">
        <span style="position:absolute; left:0; top:7px; width:5px; height:5px; background:#007acc; border-radius:50%;"></span>
        NeurIPS 2026
      </li>
      -->
    </ul>
  </li>
</ul>

<!--<h2><font face="Arial"> Professional Activities </font></h2>-->
<!--<ul style="list-style-type:none"> -->
<!--	 <li> <p style="margin-left: 0px; line-height: 120%; margin-top: 8px; margin-bottom: 8px;"><font face="Arial" size="3"><meta charset="utf-8">-->
<!--     <strong> Journal Reviewer </strong> <br> </font> </p>-->
<!--     <strong> Conference Reviewer </strong> <br> </font> </p>-->
<!--&lt;!&ndash;     ICLR 2026&ndash;&gt;-->
<!--&lt;!&ndash;   <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">&ndash;&gt;-->
<!--&lt;!&ndash;        The Conference on Neural Information Processing Systems (NeurIPS) 2024 </p></li>&ndash;&gt;-->
<!--&lt;!&ndash;    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">&ndash;&gt;-->
<!--&lt;!&ndash;        International Conference on Learning Representations (ICLR) 2024 </p></li>&ndash;&gt;-->
<!--&lt;!&ndash;    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">&ndash;&gt;-->
<!--&lt;!&ndash;        IEEE International Conference on Computer Vision (ICCV) 2023, 2021 </p></li>&ndash;&gt;-->
<!--&lt;!&ndash;    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">&ndash;&gt;-->
<!--&lt;!&ndash;        European Conference on Computer Vision (ECCV) 2024, 2022 </p></li>&ndash;&gt;-->
<!--&lt;!&ndash;    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">&ndash;&gt;-->
<!--&lt;!&ndash;        IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2022, 2023 </p></li>&ndash;&gt;-->
<!--&lt;!&ndash;    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">&ndash;&gt;-->
<!--&lt;!&ndash;        AAAI Conference on Artificial Intelligence (AAAI) 2022, 2023, 2024 </p></li>&ndash;&gt;-->
<!--&lt;!&ndash;    <li> <p style="margin-left: 10px; line-height: 150%; margin-top: -6px; margin-bottom: -6px;">&ndash;&gt;-->
<!--&lt;!&ndash;        ACM International Conference on Multimedia (ACM MM) 2021, 2023 </p></li>&ndash;&gt;-->
<!--</ul>-->
<!--		-->

<div id="footer">
	<div id="footer-text"></div>
</div>

<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=487&t=tt&d=cKc7Wx7G5hG6Og0yBAYJLS_gR5Sc2FMlfnB9zn0pRhE&co=7897ae&ct=ffffff&cmo=3be39e&cmn=ff5353'></script>

<!--	<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=777777&w=487&t=tt&d=xeJu_Kwek6AfO5eDCKFQ1iDWjzFQPLT_dNcYY3WLmrY&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=5b1717'></script>-->
<p></p><center>
<br>
    ¬© Keming Wu | Last updated: 2025/10/6
</center><p></p>
<!--</b></b></b></div><b><b><b>-->

</b></b></b></body></html>
